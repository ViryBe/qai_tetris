\documentclass[tikz, footheight=2em]{beamer}
\usetheme[hideothersubsections]{Hannover}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{color,graphicx}
\usepackage{gensymb}
\usepackage{tikz,pgfplots}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{subfig}
\usepackage{tabulary}

\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\uniform}{Uniform}

\usecolortheme{dolphin}

\title{QAI Tetris}
\author{C.~Cousin, G.~Hondet, L.~Pineau, B.~Viry}
\date{}

\usetikzlibrary{patterns,arrows,positioning}
\tikzset{
    %Define standard arrow tip
    >=stealth',
    %Define style for boxes
    punkt/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           text width=6.5em,
           minimum height=2em,
           text centered},
    % Define arrow style
    pildwn/.style={
           <-,
           thick,
           shorten <=2pt,
           shorten >=2pt,},
    pilup/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}

% ----------------------------------------------------------
% nuremotation des pages -----------------------------------
% ----------------------------------------------------------
\def\swidth{1.6cm}
\setbeamersize{sidebar width left=\swidth}
\setbeamertemplate{sidebar left}
{%
  {\usebeamerfont{title in sidebar}
    \vskip1.5em
    \usebeamercolor[fg]{title in sidebar}
    \insertshorttitle[width=\swidth,center,respectlinebreaks]\par
    \vskip1.25em
  }
  {
    \usebeamercolor[fg]{author in sidebar}
    \usebeamerfont{author in sidebar}
    \insertshortauthor[width=\swidth,center,respectlinebreaks]\par
    \vskip1.25em
  }
  \hbox to2cm{\hss\insertlogo\hss}
  \vskip1.25em
  \insertverticalnavigation{\swidth}
  \vfill
  \hbox to2cm{\hskip0.6cm\usebeamerfont{subsection in
      sidebar}\strut\usebeamercolor[fg]{subsection in
      sidebar}\insertframenumber /\inserttotalframenumber\hfill}
  \vskip3pt
}
% ----------------------------------------------------------


\begin{document}

\frame{\titlepage}

\AtBeginSection[]
{%
  \begin{frame}
    \frametitle{Plan}
    \tableofcontents[currentsection]
  \end{frame}
}

\section*{Introduction}
\begin{frame}[c]{Introduction}
\end{frame}

\section{Apprentissage}
\subsection{Version}
\begin{frame}{Apprentissage par renforcement}
  \begin{block}{Principe}
    Un agent évoluant dans son environnement se voit attribuer des
    récompenses en fonction de la pertinence de ses actions.
    \begin{figure}[h]
      \begin{center}
        \begin{tikzpicture}[node distance=1cm, auto,]
         %nodes
          \node[punkt] (env) {Environnement};
          \node[punkt, inner sep=5pt,below=2cm of env]
          (agent) {Agent}
          edge[pilup,bend right=60] node[right] {Action} (env.east)
          edge[pildwn,bend left=70] node[left] {Nouvel état} (env.west)
          edge[pildwn,bend left=50] node[right] {Récompense} (env.west);
        \end{tikzpicture}
      \end{center}
      \caption{Principe d'apprentissage par renforcement}
      \label{fig:reinforcement_learning}
    \end{figure}
  \end{block}
\end{frame}


\subsection{Méthode employée}

\begin{frame}{Q learning}
  \begin{block}{Agent}
    Utilise une matrice Q pour stocker les espérances de récompense.
  \end{block}
  \begin{itemize}
    \item Les colonnes de la matrice correspondent aux actions.
    \item Les lignes de la matrice correspondent aux états.
  \end{itemize}
\end{frame}

\subsection{Choix de l'action}

\begin{frame}{État}
  Données utiles de l'environnement pour jouer une action.

  \begin{block}{Obtention}
    \[
      \text{plateau} \pause{} + \text{pièce} \pause{} = \text{état}
      \pause{} \in \mathbf{N}
    \]
  \end{block}
\end{frame}

\begin{frame}{Choix de l'action}
    \(Q(s, a)\) approxime la récompense a long terme,
    \pause{}
    \[ \text{action choisie} = \argmax_a Q(s, a) \]
    \pause{}
  \begin{algorithm}[H]
      \caption{Choix de l'action}\label{alg:action}
      \begin{algorithmic}
        [1]
        \Procedure{ChooseAction}{$Q$, $s$, $\mathcal{A}$}
        \State{} tirage \(\gets \uniform([0, 1])\)
        \If{tirage \(> \epsilon\)}
        \Return{\(\argmax_{a\in\mathcal{A}} Q(s, a)\)}
        \Else{}
        \Return{\(\uniform(\mathcal{A})\)}
        \EndIf{}
        \EndProcedure{}
      \end{algorithmic}
  \end{algorithm}
\end{frame}

\section{Environnement Tetris}
\subsection{Tetromino}
\begin{frame}[c]{Tetromino}
  \begin{block}{Représentation}
    Matrice \(2 \times 2\) d'entiers
  \end{block}
  \begin{figure}[h]
    \centering
    \subfloat[Diagonale] {%
      \begin{tikzpicture}
        \fill[color=magenta] (0,0) rectangle (1,1);
        \fill[color=magenta] (0,0) rectangle (-1,-1);
        \draw (-1,1) rectangle (1,-1);
      \end{tikzpicture}
    }\qquad
    \subfloat[L] {%
      \begin{tikzpicture}
        \fill[color=cyan] (0,0) rectangle (1,1);
        \fill[color=cyan] (0,0) rectangle (-1,-1);
        \fill[color=cyan] (0,0) rectangle (-1,1);
        \draw (-1,1) rectangle (1,-1);
      \end{tikzpicture}
    }\qquad
    \subfloat[Carré] {%
      \begin{tikzpicture}
        \fill[color=yellow] (0,0) rectangle (1,1);
        \fill[color=yellow] (0,0) rectangle (-1,-1);
        \fill[color=yellow] (0,0) rectangle (-1,1);
        \fill[color=yellow] (0,0) rectangle (1,-1);
        \draw (-1,1) rectangle (1,-1);
      \end{tikzpicture}
    }\\
    \subfloat[Barre] {%
      \begin{tikzpicture}
        \fill[color=green] (0,0) rectangle (-1,-1);
        \fill[color=green] (0,0) rectangle (-1,1);
        \draw (-1,1) rectangle (1,-1);
      \end{tikzpicture}
    }\qquad
    \subfloat[Point] {%
      \begin{tikzpicture}
        \fill[color=red] (0,0) rectangle (-1,1);
        \draw (-1,1) rectangle (1,-1);
      \end{tikzpicture}
    }
    \caption{Liste des tetrominos}\label{fig:tetrolist}
  \end{figure}
\end{frame}

\subsection{Mouvement}
\begin{frame}[c]{Mouvement}
  \begin{block}{Définition}
    Données utiles au placement d'un tetromino sur le plateau.
    \[ \text{action} = \text{orientation} + \text{translation} \]
  \end{block}

  \begin{block}{Types}
    \begin{itemize}
      \item translation \(\in [0, 4]\)
      \item orientation \(\in \{\)North, South, East, West\(\}\)
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Gestion du plateau}
\begin{frame}[c]{Réalisation du mouvement et conséquences}
  \begin{block}{Placement vertical}
    Évaluation des collisions: collision \(\rightarrow\) placé en amont
  \end{block}
  \begin{block}{Lignes pleines}
    Ligne remplie enlevée du jeu
  \end{block}
  \begin{alertblock}{Tetromino flottant}
    schema here. Un ensemble d'orientations par tetromino.
  \end{alertblock}
\end{frame}

\section{Évolution de l'agent}
\subsection{Récompense}
\begin{frame}[c]{Récompense ou pénalité}
  \begin{block}{Motivation}
    Quantifie la pertinence du mouvement effectué
  \end{block}
  \pause{}
  \begin{block}{Objectif de l'agent}
    Minimiser la hauteur finale
  \end{block}
  \[ R(\delta h) = -100 \cdot \delta h \]
\end{frame}
\subsection{Mise à jour}
\begin{frame}[c]{Mise à jour}
  \begin{block}{Besoin}
    Prendre en compte les récompenses pour modifier le comportement de l'agent.
  \end{block}
  \[
    Q(s, a) \leftarrow Q(s,a) + \alpha
    [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
  \]
\end{frame}

\subsection{Paramétrage}
\begin{frame}[c]{Paramètres}
  \begin{block}{Taux d'apprentissage \(\alpha\)}
    Doit vérifier \(\sum_{k} a_k = \infty, \quad \sum_k a_k < \infty \). Ici,
    \[ \alpha_k = \frac{1}{1 + Ck} \]
    Nous agirons sur \(C\).
  \end{block}

  \begin{block}{Vision de l'agent \(\gamma\)}
    Prise en compte des récompenses futures, \(\in [0,1]\).
  \end{block}
\end{frame}

\begin{frame}[c]{Exploitation v exploration}
  \begin{description}
    \item[Exploration] balayer toutes les actions possibles
    \item[Exploitation] tirer profit de l'apprentissage
  \end{description}
  \[ \Downarrow \]
  \[ \text{Choix de } \epsilon \]
  \begin{block}{Valeurs}
  \end{block}
\end{frame}

\begin{frame}[c]{Qualification de l'agent}
  \begin{description}
    \item[Vitsse de décroissance] temps de passage au-dessous d'un seuil
    \item[Hauteur finale] hauteur une fois celle ci stabilisée
    \item[Stabilité] variation des hauteurs finales autour de la moyenne 
  \end{description}
  + graph
\end{frame}

\begin{frame}[c]{Résultats}
  \begin{figure}[H]
    \begin{tikzpicture}
      \begin{axis}[line width=0.05, mark size = 0.1,
          legend entries={.002,.068,.134,.2},
          xlabel = {Jeu},
          ylabel = {Hauteur fin de jeu},
          title = Apprentissages avec différents \(\epsilon\),
        ]
        \foreach \i in {0,1,...,3}{%
          \addplot+ table [x expr = {\lineno}, y index = \i]
          {data/epsilon_wide.dat};
        }
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}[c]{In fine}
  \begin{block}{Protocole}
    \begin{itemize}
      \item Seuil: nombre de jeux pour passer en dessous d'une hauteur finale
        fixée.
      \item Moyenne (moy.) et écarts types (e.t.) calcules sur les dernières
        parties.
    \end{itemize}
  \end{block}
  \begin{table}
    \centering
    \begin{tabulary}{\linewidth}{|C|r|r|r||r|r|r|}
      \hline
      & \(\alpha\) & \(\epsilon\) & \(\gamma\) & Moy. & E.t.\ & Seuil\\
      \hline
      Moy. & 0.001 & 0.0025 & 0.65 & 1.14 & 1.08 & 68\\
      \hline
      E.t.\ & 0.001 & 0.0025 & 0.65 & 1.14 & 1.08 & 68\\
      \hline
      Seuil & 0.0007 & 0.0005 & 1 & 3.14 & 2.96 & 29\\
      \hline
    \end{tabulary}
    \caption{Valeurs optimales}\label{tab:param}
  \end{table}
\end{frame}


\section{Valuation}
\begin{frame}[c]{Fonction de valuation}
  \begin{block}{Motivation}
    S'affranchir des contraintes mémoires liées à la Q matrice.
  \end{block}
  \pause{}
  \begin{block}{Agent}
    Application linéaire \(V\) donnant l'espérance de récompense.
  \end{block}
  \pause{}
  \begin{exampleblock}{Features}
    Caractéristiques utiles du plateau pour estimer l'espérance de récompense.
    E.g.\ Nombre de trous, hauteur de la dernière pièce tombée\dots
  \end{exampleblock}
  \pause{}
  \begin{block}{Apprentissage}
    Modification des poids \(w\) composant \(V\) par descente de gradient.
  \end{block}
\end{frame}

\section*{Conclusion}
\begin{frame}[c]{Conclusion}
  Résultats concluant concernant le Q learning.
\end{frame}
\end{document}
